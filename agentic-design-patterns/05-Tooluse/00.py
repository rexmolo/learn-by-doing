import os, getpass
import asyncio
import nest_asyncio
from typing import List
from dotenv import load_dotenv
import logging

from langchain_openai import ChatOpenAI
from langchain_core.tools import tool as langchain_tool
from langgraph.prebuilt import create_react_agent

# Load environment variables from .env file
load_dotenv()

# UNCOMMENT if you want to enter the API key manually
# os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")

try:
   # A model with function/tool calling capabilities is required.
   # éœ€è¦ä¸€ä¸ªå…·æœ‰å‡½æ•°è°ƒç”¨èƒ½åŠ›çš„æ¨¡å‹ï¼Œè¿™é‡Œä½¿ç”¨ Gemini 2.0 Flashã€‚
   llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
   print(f"âœ… Language model initialized: {llm.model_name}")
except Exception as e:
   print(f"ğŸ›‘ Error initializing language model: {e}")
   llm = None

# --- Define a Tool ---
# --- å®šä¹‰æ¨¡æ‹Ÿçš„æœç´¢å·¥å…· ---
@langchain_tool
def search_information(query: str) -> str:
   """
   Provides factual information on a given topic. Use this tool to find answers to phrases
   like 'capital of France' or 'weather in London?'.
   # æ¨¡æ‹Ÿæä¾›å…³äºç‰¹å®šæŸ¥è¯¢çš„è¾“å‡ºã€‚ä½¿ç”¨æ­¤å·¥å…·æŸ¥æ‰¾ç±»ä¼¼ã€Œæ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿã€æˆ–ã€Œä¼¦æ•¦çš„å¤©æ°”å¦‚ä½•ï¼Ÿã€è¿™ç±»é—®é¢˜çš„ç­”æ¡ˆã€‚
   """
   print(f"\n--- ğŸ› ï¸ Tool Called: search_information with query: '{query}' ---")
   # Simulate a search tool with a dictionary of predefined results.
   # é€šè¿‡ä¸€ä¸ªå­—å…¸é¢„å®šä¹‰çš„ç»“æœæ¥æ¨¡æ‹Ÿæœç´¢å·¥å…·ã€‚
   simulated_results = {
       "weather in london": "The weather in London is currently cloudy with a temperature of 15Â°C.",
       "capital of france": "The capital of France is Paris.",
       "population of earth": "The estimated population of Earth is around 8 billion people.",
       "tallest mountain": "Mount Everest is the tallest mountain above sea level.",
       "default": f"Simulated search result for '{query}': No specific information found, but the topic seems interesting."
   }
   result = simulated_results.get(query.lower(), simulated_results["default"])
   print(f"--- TOOL RESULT: {result} ---")
   return result

tools = [search_information]

# --- Create a Tool-Calling Agent ---
# --- åˆ›å»ºä¸€ä¸ªä½¿ç”¨å·¥å…·çš„æ™ºèƒ½ä½“ ---
if llm:
   # Create the agent using langgraph's create_react_agent.
   # This returns a compiled graph that can be invoked directly.
   # ä½¿ç”¨ langgraph çš„ create_react_agent åˆ›å»ºæ™ºèƒ½ä½“ã€‚
   # è¿™å°†è¿”å›ä¸€ä¸ªå¯ä»¥ç›´æ¥è°ƒç”¨çš„ç¼–è¯‘å›¾ã€‚
   agent_executor = create_react_agent(llm, tools)

async def run_agent_with_tool(query: str):
   """
   Invokes the agent executor with a query and prints the final response.
   æ‰§è¡Œæ™ºèƒ½ä½“å¹¶æ‰“å°æœ€ç»ˆè¾“å‡ºä¿¡æ¯ã€‚
   """
   print(f"\n--- ğŸƒ Running Agent with Query: '{query}' ---")
   try:
       # Langgraph agents expect 'messages' as input
       # Langgraph æ™ºèƒ½ä½“æœŸæœ› 'messages' ä½œä¸ºè¾“å…¥
       response = await agent_executor.ainvoke({"messages": [("user", query)]})
       print("\n--- âœ… Final Agent Response ---")
       # Get the last message content from the response
       # ä»å“åº”ä¸­è·å–æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
       final_message = response["messages"][-1].content
       print(final_message)
   except Exception as e:
       print(f"\nğŸ›‘ An error occurred during agent execution: {e}")

async def main():
   """
   Runs all agent queries concurrently.
   å¹¶å‘è¿è¡Œæ‰€æœ‰æ™ºèƒ½ä½“æŸ¥è¯¢ä»»åŠ¡ã€‚
   """
   tasks = [
       run_agent_with_tool("What is the capital of France?"),
       run_agent_with_tool("What's the weather like in London?"),
       run_agent_with_tool("Tell me something about dogs.") # Should trigger the default tool response
   ]
   await asyncio.gather(*tasks)

if llm:
   nest_asyncio.apply()
   asyncio.run(main())
else:
   print("\nâŒ Cannot run agent: LLM was not initialized. Please set OPENAI_API_KEY in your .env file.")